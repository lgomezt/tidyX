{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidytweets\n",
    "Clean text extracted from social networks to perform various NLP tasks such as topic analysis, word embeddings, sentiment analysis, etc.\n",
    "\n",
    "## Table of contents\n",
    "1. `remove_repetitions`\n",
    "2. `remove_last_repetition`\n",
    "3. `remove_urls`\n",
    "4. `remove_RT`\n",
    "5. `remove_accents`\n",
    "6. `remove_hashtags`\n",
    "7. `remove_mentions`\n",
    "8. `remove_special_characters`\n",
    "9. `remove_extra_spaces`\n",
    "10. `space_between_emojis`\n",
    "11. `preprocess`\n",
    "12. `remove_words`\n",
    "13. `unnest_tokens`\n",
    "14. `spanish_lemmatizer`\n",
    "15. `create_bol`\n",
    "16. Tutorial: Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_terminal_size' from 'click.termui' (C:\\Users\\JOSE\\AppData\\Roaming\\Python\\Python39\\site-packages\\click\\termui.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-54d7b0efee57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr'C:\\Users\\JOSE\\Desktop\\Trabajo\\Paper_no_supervisado\\Tidytweets'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Import tidytweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtidyX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextPreprocessor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Trabajo\\Paper_no_supervisado\\Tidytweets\\tidyX\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtidyX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextPreprocessor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextPreprocessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Trabajo\\Paper_no_supervisado\\Tidytweets\\tidyX\\TextPreprocessor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpanish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexplain\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\cli\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwasabi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetup_cli\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# These are the actual functions, NOT the wrapped CLI commands. The CLI commands\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\cli\\_util.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtyper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNoSuchOption\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msplit_arg_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\typer-0.3.2-py3.9.egg\\typer\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mecho_via_pager\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mecho_via_pager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0medit\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0medit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_terminal_size\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mget_terminal_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgetchar\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgetchar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlaunch\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlaunch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_terminal_size' from 'click.termui' (C:\\Users\\JOSE\\AppData\\Roaming\\Python\\Python39\\site-packages\\click\\termui.py)"
     ]
    }
   ],
   "source": [
    "# Set-up\n",
    "import sys\n",
    "#sys.path.insert(1, r'C:\\Users\\Lucas\\Documents\\Tidytweets')\n",
    "#sys.path.insert(1, r'C:\\Users\\Lucas\\Documents\\Tidytweets\\tidytweets')\n",
    "sys.path.insert(1, r'C:\\Users\\JOSE\\Desktop\\Trabajo\\Paper_no_supervisado\\Tidytweets')\n",
    "# Import tidytweets \n",
    "import tidyX.TextPreprocessor as tt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `remove_repetitions`\n",
    "**Description of the function**\n",
    "\n",
    "This function deletes any consecutive repetition of characters in a string. For example, the string 'coooroosooo' will be changed to 'coroso'. As in many languages it's common to have some special characters that can be repeated, for example the 'l' in spanish to form 'll', the exception argument could be used to specify which characters are allowed to repeat once.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "In social media, it is common for people to repeat certain characters of a word in order to add more emotion to a sentence. However, when we attempt to count the occurrences of a word, the various ways in which a word can be written make it difficult to uniquely identify each instance. One simple solution to this issue is to use the `remove_repetitions` function. Let's consider the following tweet:\n",
    "\n",
    "<center>\n",
    "<img src=\"img/remove_repetitions1.png\" alt=\"remove_repetitions1\" height=300px />\n",
    "</center>\n",
    "\n",
    "In this particular case, the author writes \"Goooal\" and \"Goal.\" Consequently, it becomes necessary for us to eliminate the repeated \"o\"s in the first word in order to make both words equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Goooal ‚öΩÔ∏è‚öΩÔ∏è‚öΩÔ∏è Christiano Ronaldo Amazing Goal Juventus vs Real Madrid 1-3 Champions League Final #JUVRMA #UCLFinal2017 #JuventusRealMadrid\n"
     ]
    }
   ],
   "source": [
    "string_example = \"Goooal ‚öΩÔ∏è‚öΩÔ∏è‚öΩÔ∏è Christiano Ronaldo Amazing Goal Juventus vs Real Madrid 1-3 Champions League Final #JUVRMA #UCLFinal2017 #JuventusRealMadrid\"\n",
    "print(\"Before:\", string_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: Goal ‚öΩÔ∏è‚öΩÔ∏è‚öΩÔ∏è Christiano Ronaldo Amazing Goal Juventus vs Real Madrid 1-3 Champions League Final #JUVRMA #UCLFinal2017 #JuventusRealMadrid\n"
     ]
    }
   ],
   "source": [
    "string_without_repetitions = tt.remove_repetitions(string = string_example, exceptions = None)\n",
    "print(\"After:\", string_without_repetitions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's worth noting that there exist numerous words that feature the repetition of a single character. To address this, the `remove_repetitions` function incorporates the `exceptions` parameter, which allows for specifying a list of characters that are permitted to appear twice. For instance, if we set `exceptions = ['p']`, words such as 'happpy' will be cleaned and transformed into 'happy'. The default value for this parameter is `['r', 'l', 'n', 'c', 'a', 'e', 'o']`. Let's see another example:\n",
    "\n",
    "<center>\n",
    "<img src=\"img/remove_repetitions2.png\" alt=\"remove_repetitions2\" width=300px />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: HAPPPYYYYY GRADUATION TO US!! THANKYOUUUU LORD!!! ü´∂ü§ç\n"
     ]
    }
   ],
   "source": [
    "string_example = \"HAPPPYYYYY GRADUATION TO US!! THANKYOUUUU LORD!!! ü´∂ü§ç\"\n",
    "print(\"Before:\", string_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: HAPPY GRADUATION TO US! THANKYOU LORD! ü´∂ü§ç\n"
     ]
    }
   ],
   "source": [
    "string_without_repetitions = tt.remove_repetitions(string = string_example, exceptions = [\"P\"])\n",
    "print(\"After:\", string_without_repetitions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `remove_last_repetition`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `remove_last_repetition` function is designed to remove the repetition of the last character in each word of a given string. It's particularly useful when dealing with text that contains repeated characters at the end of words, a common occurrence in social media posts where users emphasize words for expression. This function helps clean and standardize the text by eliminating these last-character repetitions.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "Imagine you are analyzing text data from social media platforms, and you want to ensure consistency in your analysis by removing repetitive characters at the end of words. For example, in Spanish, words typically do not end with a repeated character, but social media users often add emphasis by repeating the last character. Let's explore a practical use case with a tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Holaaaa amigooo\n",
      "After: Hola amigo\n"
     ]
    }
   ],
   "source": [
    "# Original tweet with last-character repetitions\n",
    "string_example = \"Holaaaa amigooo\"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# Apply remove_last_repetition function to clean the text\n",
    "string_without_last_repetitions = tt.remove_last_repetition(string=string_example)\n",
    "print(\"After:\", string_without_last_repetitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input string contains repeated characters at the end of words, like \"Holaaaa\" and \"amigooo.\" To ensure consistent analysis, you can use the `remove_last_repetition` function, which removes the last-character repetitions and transforms the text into \"Hola amigo.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `remove_urls`\n",
    "**Description of the function**\n",
    "\n",
    "The `remove_urls` function is designed to remove all URLs that start with \"http\" from a given string. It's a handy tool for text processing when you want to eliminate URLs from a text dataset, making it cleaner and more focused on textual content. This function scans the entire string, identifies any sequences of characters that start with \"http\" and continue until a space or end of the line, and removes them.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "You may encounter situations where you want to analyze or visualize the textual content of a dataset, but the presence of URLs can clutter the text and skew your analysis. This is especially common in social media data, chat messages, or web scraping scenarios. Let's explore a practical use case with a sample text containing URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Check out our website: http://example.com. For more info, visit http://example2.com\n",
      "After: Check out our website:  For more info, visit \n"
     ]
    }
   ],
   "source": [
    "# Original text with URLs\n",
    "string_example = \"Check out our website: http://example.com. For more info, visit http://example2.com\"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# Apply remove_urls function to clean the text\n",
    "string_without_urls = tt.remove_urls(string=string_example)\n",
    "print(\"After:\", string_without_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input string contains two URLs, \"http://example.com\" and \"http://example2.com.\" To focus on the textual content without the distraction of URLs, you can use the `remove_urls` function, which removes them and results in cleaner text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `remove_RT`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `remove_RT` function is designed to remove the \"RT\" prefix from tweets. In the context of social media, \"RT\" typically stands for \"Retweet\" and is often used as a prefix when users share or retweet content. This function is useful for cleaning and standardizing tweet text data by removing the \"RT\" prefix, accounting for varying amounts of white space after \"RT.\"\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "When you're working with tweet data and you want to analyze or visualize the content of tweets without the distraction of the \"RT\" prefix, the remove_RT function comes in handy. Retweets often have the \"RT\" prefix at the beginning, but the amount of white space after \"RT\" can vary. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: RT     @username: Check out this amazing article!\n",
      "After: @username: Check out this amazing article!\n"
     ]
    }
   ],
   "source": [
    "# Original tweet with \"RT\" prefix\n",
    "string_example = \"RT     @username: Check out this amazing article!\"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# Apply remove_RT function to clean the tweet\n",
    "cleaned_tweet = tt.remove_RT(string=string_example)\n",
    "print(\"After:\", cleaned_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input tweet contains the \"RT\" prefix followed by varying amounts of white space before the actual content of the tweet. To focus on the tweet's content and remove the \"RT\" prefix, you can use the `remove_RT` function, which standardizes the text and results in a tweet without the \"RT\" prefix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. `remove_accents`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `remove_accents` function serves two purposes: it removes accent marks from characters in a given string and can optionally remove emojis. Accent marks can be common in languages like French or Spanish (this specific use case), and removing them can be helpful for text processing tasks. This function provides flexibility by allowing you to choose whether to remove emojis as well.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "The `remove_accents` function is particularly useful when working with text data that contains accented characters, and you want to simplify the text for analysis or comparison. Additionally, if your text data includes emojis that are not relevant to your analysis, you can choose to remove them as well. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Caf√© ‚òïÔ∏è √† c√¥t√© de l'h√¥tel. üòÉ\n",
      "After: Cafe  a cote de l'hotel. \n"
     ]
    }
   ],
   "source": [
    "# Original text with accents and emojis\n",
    "string_example = \"Caf√© ‚òïÔ∏è √† c√¥t√© de l'h√¥tel. üòÉ\"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# Apply remove_accents function to clean the text (removing emojis)\n",
    "cleaned_text = tt.remove_accents(string=string_example, delete_emojis=True)\n",
    "print(\"After:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input text contains accented characters (e.g., \"√©\") and emojis (e.g., \"‚òïÔ∏è\" and \"üòÉ\"). To simplify the text for analysis and remove emojis, you can use the `remove_accents` function with the `delete_emojis` option set to True, resulting in cleaned text without accents or emojis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is flexible over the total number of followed emojis on a text, let's process a Spanish common example:\n",
    "<center>\n",
    "<img src=\"img/remove_accents.png\" alt=\"remove_accents\" width=500px />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ‚ÄºÔ∏è La funci√≥n de traductor no funciona as√≠ que este tweet es solo para nuestros seguidores hispanohablantes, siempre van a ser nuestros favoritos y ahora vamos a poner emojis tristes para que los que no hablan espa√±ol se preocupen üò≠  y tambi√©n esta foto fuera de contexto üòîüíî\n",
      "After: !! La funcion de traductor no funciona asi que este tweet es solo para nuestros seguidores hispanohablantes, siempre van a ser nuestros favoritos y ahora vamos a poner emojis tristes para que los que no hablan espanol se preocupen   y tambien esta foto fuera de contexto \n"
     ]
    }
   ],
   "source": [
    "# Original text with accents and emojis\n",
    "string_example = \"‚ÄºÔ∏è La funci√≥n de traductor no funciona as√≠ que este tweet es solo para nuestros seguidores hispanohablantes, siempre van a ser nuestros favoritos y ahora vamos a poner emojis tristes para que los que no hablan espa√±ol se preocupen üò≠  y tambi√©n esta foto fuera de contexto üòîüíî\"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# Apply remove_accents function to clean the text (removing emojis)\n",
    "cleaned_text = tt.remove_accents(string=string_example, delete_emojis=True)\n",
    "print(\"After:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, the method removed continuously repeated emojis, but passes over \"!!\" v2 class emojis (Link to the emoji: https://abs-0.twimg.com/emoji/v2/svg/203c.svg). This is due to the fact that it is considered an expression, rather not a direct emoji, when you type double exclamation on Twitter. You can see a full list of this wildcard emoji converter expressions on X's documentation in https://twemoji.twitter.com/ and some examples in https://twitter.com/FakeUnicode/status/1251505174348095488"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. `remove_hashtags`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `remove_hashtags` function is designed to remove hashtags from a given string. In social media and text data, hashtags are often used to categorize or highlight content. This function scans the input string and removes any text that starts with a '#' and is followed by alphanumeric characters, effectively removing hashtags from the text.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "You might encounter situations where you want to analyze or visualize text data without the presence of hashtags. Hashtags can be prevalent in social media posts and may not be relevant to your analysis. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Exploring the beauty of #nature in #springtime. #NaturePhotography üåº\n",
      "After: Exploring the beauty of  in .  üåº\n"
     ]
    }
   ],
   "source": [
    "# Original text with hashtags\n",
    "string_example = \"Exploring the beauty of #nature in #springtime. #NaturePhotography üåº\"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# Apply remove_hashtags function to clean the text\n",
    "cleaned_text = tt.remove_hashtags(string=string_example)\n",
    "print(\"After:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input text contains hashtags such as \"#nature,\" \"#springtime,\" and \"#NaturePhotography.\" To focus on the textual content without the distraction of hashtags, you can use the `remove_hashtags` function, which removes them and results in a cleaner text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. `remove_mentions`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `remove_mentions` function is designed to remove mentions (e.g., @username) from a given tweet string. In the context of social media, mentions are often used to reference or tag other users. This function scans the input tweet string and removes any text that starts with '@' followed by a username. Optionally, it can also return a list of unique mentions found in the tweet.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "You may encounter situations where you want to analyze or visualize tweet text data without the presence of mentions. Mentions can be common in social media posts and may not be relevant to your analysis. Additionally, you might want to extract and track mentioned accounts separately. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Exploring the beauty of nature with @NatureExplorer and @WildlifeEnthusiast. #NaturePhotography üåº\n",
      "After: Exploring the beauty of nature with  and . #NaturePhotography üåº\n",
      "Extracted Mentions: ['@NatureExplorer', '@WildlifeEnthusiast']\n"
     ]
    }
   ],
   "source": [
    "# Original tweet with mentions\n",
    "string_example = \"Exploring the beauty of nature with @NatureExplorer and @WildlifeEnthusiast. #NaturePhotography üåº\"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# Apply remove_mentions function to clean the tweet and extract mentions\n",
    "cleaned_text, extracted_mentions = tt.remove_mentions(string=string_example, extract=True)\n",
    "print(\"After:\", cleaned_text)\n",
    "print(\"Extracted Mentions:\", extracted_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input tweet text contains mentions such as \"@NatureExplorer\" and \"@WildlifeEnthusiast.\" To focus on the textual content without the distraction of mentions and to extract mentioned accounts, you can use the `remove_mentions` function, which removes mentions and provides a list of unique mentions found in the tweet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. `remove_special_characters`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `remove_special_characters` function is designed to remove all characters from a string except for lowercase letters and spaces. It's a useful tool for cleaning text data when you want to focus on the textual content while excluding punctuation marks, exclamation marks, special characters, numbers, and uppercase letters. This function scans the input string and removes any character that does not match the criteria.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "You may encounter situations where you want to preprocess text data and eliminate special characters and non-lowercase characters to make it more suitable for natural language processing tasks. Cleaning text in this way can help improve text analysis, topic modeling, or sentiment analysis. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: This is an example text! It contains special characters. 123\n",
      "After: his is an example text t contains special characters \n"
     ]
    }
   ],
   "source": [
    "string_example = \"This is an example text! It contains special characters. 123\"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# Apply remove_special_characters function to clean the text\n",
    "cleaned_text = tt.remove_special_characters(string=string_example)\n",
    "print(\"After:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input text contains special characters, punctuation marks, numbers, and uppercase letters. To focus on the textual content with lowercase letters and spaces only, you can use the `remove_special_characters` function, which removes the undesired characters and results in a cleaner text. Beware to lowercase your text before applying this method over your corpus, as you can see on the past example, it can remove useful strings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. `remove_extra_spaces`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `remove_extra_spaces` function is designed to remove extra spaces within and surrounding a given string. It's a valuable tool for cleaning text data when you want to standardize spaces, trim leading and trailing spaces, and replace consecutive spaces between words with a single space. This function helps improve the consistency and readability of text.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "You may encounter situations where you want to preprocess text data and ensure consistent spacing for better readability and analysis. Extra spaces can be common in unstructured text, and cleaning them can enhance text analysis, especially when dealing with natural language processing tasks. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: This is    an   example  text with extra   spaces.     \n",
      "After: This is an example text with extra spaces.\n"
     ]
    }
   ],
   "source": [
    "# Original text with extra spaces\n",
    "string_example = \"This is    an   example  text with extra   spaces.     \"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# Apply remove_extra_spaces function to clean the text\n",
    "cleaned_text = tt.remove_extra_spaces(string=string_example)\n",
    "print(\"After:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input text contains extra spaces between words and leading/trailing spaces. To standardize the spacing and remove the extra spaces, you can use the `remove_extra_spaces` function, which trims leading/trailing spaces and replaces consecutive spaces with a single space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. `space_between_emojis`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `space_between_emojis` function is designed to insert spaces around emojis within a given string. It ensures that emojis are separated from other text or emojis in the string. This function is helpful for improving the readability of text containing emojis and ensuring proper spacing. It also removes any extra spaces resulting from the insertion of spaces around emojis.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "This function is particularly useful when you're working with text data that includes emojis and you want to enhance the visual presentation of the text. Emojis are often used for expressing emotions or conveying messages, and proper spacing ensures that emojis are distinct and do not run together. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: I loveüòçthis placeüå¥It's amazing!üëè\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'emoji' has no attribute 'UNICODE_EMOJI'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-0158916aa645>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Apply space_between_emojis function to add spaces around emojis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mcleaned_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspace_between_emojis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstring_example\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleaned_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trabajo\\Paper_no_supervisado\\Tidytweets\\tidyX\\TextPreprocessor.py\u001b[0m in \u001b[0;36mspace_between_emojis\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mTextPreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_extra_spaces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memoji\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEMOJI_DATA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trabajo\\Paper_no_supervisado\\Tidytweets\\tidyX\\TextPreprocessor.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mTextPreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_extra_spaces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memoji\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEMOJI_DATA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'emoji' has no attribute 'UNICODE_EMOJI'"
     ]
    }
   ],
   "source": [
    "# Original text with emojis\n",
    "string_example = \"I loveüòçthis placeüå¥It's amazing!üëè\"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# Apply space_between_emojis function to add spaces around emojis\n",
    "cleaned_text = tt.space_between_emojis(string=string_example)\n",
    "print(\"After:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input text contains emojis such as \"üòç,\" \"üå¥,\" and \"üëè\" mixed with regular text. To ensure that emojis are separated from other text and from each other, you can use the `space_between_emojis` function, which inserts spaces around emojis and removes any extra spaces resulting from the insertion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. `preprocess`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `preprocess` function is a comprehensive text preprocessing tool designed to clean and standardize tweet text. It applies a series of cleaning functions to perform tasks such as removing retweet prefixes, converting text to lowercase, removing accents and emojis, extracting or removing mentions, removing URLs, hashtags, special characters, extra spaces, and consecutive repeated characters with specified exceptions. This function offers extensive text cleaning capabilities and prepares tweet text for analysis or visualization.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "The `preprocess` function is particularly useful when you're working with tweet data and need to clean and standardize the text for various text analysis tasks. Tweet text can be messy and contain various elements such as mentions, URLs, emojis, and special characters that may need to be processed and standardized. Let's explore a practical use case:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: RT @user1: I love this place! üòç Check out the link: https://example.com #travel #vacation!!!\n",
      "After: i love this place check out the link\n",
      "Extracted Mentions: ['@user1']\n"
     ]
    }
   ],
   "source": [
    "# Original tweet with various elements\n",
    "string_example = \"RT @user1: I love this place! üòç Check out the link: https://example.com #travel #vacation!!!\"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# Apply preprocess function to clean and preprocess the tweet\n",
    "cleaned_text, extracted_mentions = tt.preprocess(string=string_example, delete_emojis=True)\n",
    "print(\"After:\", cleaned_text)\n",
    "print(\"Extracted Mentions:\", extracted_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input tweet text contains retweet prefixes, mentions, emojis, URLs, hashtags, and special characters. To standardize the tweet text for analysis, you can use the `preprocess` function, which performs a series of cleaning operations to remove or extract various elements and return cleaned text and mentions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. `remove_words`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `remove_words` function is designed to remove all occurrences of specific words listed in the `bag_of_words` parameter from a given string. This function is particularly useful for removing stopwords or any other set of unwanted words from text data. It performs an exact match, meaning it will remove only the exact words listed in the `bag_of_words` and won't remove variations of those words that are not in the list.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "This function is valuable when you want to clean text data by removing specific words that are not relevant to your analysis or that you consider stopwords. It's commonly used in natural language processing tasks to improve the quality of text analysis, topic modeling, or sentiment analysis. Let's explore a practical use case:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: This is an example sentence with some unnecessary words like 'the', 'is', and 'with'.\n",
      "Stopwords to Remove: ['the', 'is', 'and', 'with']\n",
      "After: This an example sentence some unnecessary words like '', '', ''.\n"
     ]
    }
   ],
   "source": [
    "# Original text with stopwords\n",
    "string_example = \"This is an example sentence with some unnecessary words like 'the', 'is', and 'with'.\"\n",
    "print(\"Before:\", string_example)\n",
    "\n",
    "# List of stopwords to remove\n",
    "stopwords = [\"the\", \"is\", \"and\", \"with\"]\n",
    "print(\"Stopwords to Remove:\", stopwords)\n",
    "\n",
    "# Apply remove_words function to clean the text\n",
    "cleaned_text = tt.remove_words(string=string_example, bag_of_words=stopwords)\n",
    "print(\"After:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input text contains stopwords such as \"the,\" \"is,\" and \"with.\" To clean the text by removing these stopwords, you can use the `remove_words` function, which removes the specified words from the text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. `unnest_tokens`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `unnest_tokens` function is designed to flatten a pandas DataFrame by tokenizing a specified column. It takes a pandas DataFrame, the name of the column to tokenize, and an optional flag to create an \"id\" column based on the DataFrame's index. Each token in the specified column becomes a separate row in the resulting DataFrame, effectively \"exploding\" the data into a long format.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "This function is useful when you have text data stored in a DataFrame, and you want to transform it into a format that is more suitable for certain text analysis or modeling tasks. For instance, when working with natural language processing or text mining, you may need to tokenize text data and represent it in a format where each token corresponds to a separate row. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "                     text_column\n",
      "0     This is a sample sentence.\n",
      "1  Another sentence with tokens.\n",
      "2  Text analysis is interesting.\n",
      "\n",
      "Tokenized DataFrame:\n",
      "   id   text_column\n",
      "0   0          This\n",
      "0   0            is\n",
      "0   0             a\n",
      "0   0        sample\n",
      "0   0     sentence.\n",
      "1   1       Another\n",
      "1   1      sentence\n",
      "1   1          with\n",
      "1   1       tokens.\n",
      "2   2          Text\n",
      "2   2      analysis\n",
      "2   2            is\n",
      "2   2  interesting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Create a sample DataFrame with a text column\n",
    "data = {'text_column': [\"This is a sample sentence.\",\n",
    "                        \"Another sentence with tokens.\",\n",
    "                        \"Text analysis is interesting.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Apply unnest_tokens function to tokenize the text column\n",
    "tokenized_df = tt.unnest_tokens(df=df, input_column='text_column')\n",
    "print(\"\\nTokenized DataFrame:\")\n",
    "print(tokenized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the input DataFrame contains a column named 'text_column' with sentences. To tokenize the text and transform it into a long format where each token is a separate row, you can use the `unnest_tokens` function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. `spanish_lemmatizer`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `spanish_lemmatizer` function is designed to lemmatize a given Spanish language token using Spacy's Spanish language model. It takes a token (word) and a Spacy language model object as input and returns the lemmatized version of the token with accents removed. This function is valuable for text analysis tasks where you need to reduce words to their base or dictionary form.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "This function is useful when you're working with text data in Spanish and want to perform text analysis tasks such as sentiment analysis, topic modeling, or text classification. Lemmatization helps standardize words to their base form, reducing the complexity of text data. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JOSE\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\util.py:1707: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Token: corriendo\n",
      "Lemmatized Token: correr\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#!python -m spacy download es_core_news_sm # Download Spanish language model. Run this line first!\n",
    "# Load Spacy's Spanish language model (you should have this model downloaded)\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Input token to lemmatize\n",
    "token = \"corriendo\"  # Example token in Spanish\n",
    "print(\"Original Token:\", token)\n",
    "\n",
    "# Apply spanish_lemmatizer function to lemmatize the token\n",
    "lemmatized_token = tt.spanish_lemmatizer(token=token, model=nlp)\n",
    "print(\"Lemmatized Token:\", lemmatized_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have an input token, \"corriendo,\" in Spanish that we want to lemmatize to its base form. We use the `spanish_lemmatizer` function to perform the lemmatization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. `create_bol`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `create_bol` function is designed to group lemmas based on Levenshtein distance to handle misspelled words in social media data. It takes a numpy array containing lemmas and an optional verbose flag for progress reporting. The function groups similar lemmas into bags of lemmas based on their Levenshtein distance. The result is a pandas DataFrame that contains information about the bags of lemmas, including their IDs, names, associated lemmas, and the similarity threshold used for grouping.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "This function is useful when you're dealing with text data, especially social media data, where misspelled or variations of words are common. Grouping similar lemmas together can help clean and organize text data for analysis, improving the accuracy of text-based tasks like sentiment analysis or topic modeling. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Lemmas:\n",
      "['apple' 'aple' 'apples' 'banana' 'banan' 'bananas' 'cherry' 'cheri'\n",
      " 'cherries']\n",
      "An error occurred: integer division or modulo by zero\n",
      "\n",
      "Bags of Lemmas DataFrame:\n",
      "   bow_id bow_name   lemma  similarity  threshold\n",
      "0       1    apple   apple         100         86\n",
      "1       1    apple    aple          89         86\n",
      "2       1    apple  apples          91         86\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Create a numpy array of lemmas\n",
    "lemmas = np.array(['apple', 'aple', 'apples', 'banana', 'banan', 'bananas', 'cherry', 'cheri', 'cherries'])\n",
    "print(\"Original Lemmas:\")\n",
    "print(lemmas)\n",
    "\n",
    "# Apply create_bol function to group similar lemmas\n",
    "bol_df = tt.create_bol(lemmas=lemmas, verbose=True)\n",
    "print(\"\\nBags of Lemmas DataFrame:\")\n",
    "print(bol_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have an array of lemmas representing fruits, but some of the lemmas are misspelled or have variations. We want to group similar lemmas together into bags of lemmas using the `create_bol` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. `get_most_common_strings`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `get_most_common_strings` function is designed to identify and retrieve the most common strings in a list of texts. It takes two arguments: a list of texts and an integer specifying the number of most common words to return. The function calculates word frequencies across the texts and returns a list of the most frequently occurring words along with their respective counts.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "This function is particularly useful when you want to gain insights into the content of a collection of texts. It helps you identify which words or strings are the most prevalent within the text data. You can use this information for various purposes, including data validation, descriptive analysis, or identifying significant terms in text data. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TextPreprocessor' has no attribute 'get_most_common_strings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-223412499d66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Apply get_most_common_strings function to find the most common words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmost_common_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_most_common_strings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_strings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_strings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Most Common Strings:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmost_common_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'TextPreprocessor' has no attribute 'get_most_common_strings'"
     ]
    }
   ],
   "source": [
    "# List of example texts\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A quick brown dog jumps over a lazy fox.\",\n",
    "    \"The quick brown dog jumps over the quick lazy fox.\"\n",
    "]\n",
    "\n",
    "# Number of most common strings to retrieve\n",
    "num_strings = 5\n",
    "\n",
    "# Apply get_most_common_strings function to find the most common words\n",
    "most_common_words = tt.get_most_common_strings(texts=texts, num_strings=num_strings)\n",
    "print(\"Most Common Strings:\")\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have a list of example texts, and we want to find the most common words within these texts using the `get_most_common_strings` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. `spacy_pipeline`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `spacy_pipeline` function is a comprehensive text preprocessing tool that leverages spaCy's capabilities to process a list of documents. It allows you to customize the spaCy pipeline, including options such as using a custom lemmatizer for Spanish, specifying stopwords language, choosing a spaCy model, and retrieving the most common words after preprocessing.\n",
    "\n",
    "The function takes several arguments, including a list of documents, a custom lemmatizer flag, pipeline components, stopwords language, spaCy model, and the number of most common words to return. It processes the documents by tokenizing, lemmatizing, and removing stopwords, providing you with well-preprocessed documents and a list of the most common words within them.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "This function is highly useful when you need to preprocess a collection of text documents for natural language processing tasks. It offers flexibility by allowing you to configure the spaCy pipeline according to your specific requirements. Additionally, it provides insights into the most common words in the preprocessed documents, which can be valuable for data validation or descriptive analysis. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TextPreprocessor' has no attribute 'spacy_pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-758903b8f17f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Apply spacy_pipeline function to preprocess documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m processed_documents, most_common_words = tt.spacy_pipeline(\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mcustom_lemmatizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_lemmatizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'TextPreprocessor' has no attribute 'spacy_pipeline'"
     ]
    }
   ],
   "source": [
    "# List of example documents\n",
    "documents = [\n",
    "    \"El r√°pido zorro marr√≥n salta sobre el perro perezoso.\",\n",
    "    \"Un veloz perro marr√≥n salta sobre un zorro perezoso.\",\n",
    "    \"El r√°pido perro marr√≥n salta sobre el veloz zorro perezoso.\"\n",
    "]\n",
    "\n",
    "# Specify preprocessing options\n",
    "custom_lemmatizer = True\n",
    "pipeline = ['tokenize', 'lemmatizer']\n",
    "stopwords_language = 'spanish'\n",
    "model = 'es_core_news_sm'\n",
    "num_strings = 5\n",
    "\n",
    "# Apply spacy_pipeline function to preprocess documents\n",
    "processed_documents, most_common_words = tt.spacy_pipeline(\n",
    "    documents=documents,\n",
    "    custom_lemmatizer=custom_lemmatizer,\n",
    "    pipeline=pipeline,\n",
    "    stopwords_language=stopwords_language,\n",
    "    model=model,\n",
    "    num_strings=num_strings\n",
    ")\n",
    "\n",
    "print(\"Processed Documents:\")\n",
    "for i, doc in enumerate(processed_documents):\n",
    "    print(f\"Document {i + 1}: {' '.join(doc)}\")\n",
    "\n",
    "print(\"\\nMost Common Words:\")\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. `spacy_pipeline`\n",
    "\n",
    "**Description of the function**\n",
    "\n",
    "The `spacy_pipeline` function is a comprehensive text preprocessing tool that leverages spaCy's capabilities to process a list of documents. It allows you to customize the spaCy pipeline, including options such as using a custom lemmatizer for Spanish, specifying stopwords language, choosing a spaCy model, and retrieving the most common words after preprocessing.\n",
    "\n",
    "The function takes several arguments, including a list of documents, a custom lemmatizer flag, pipeline components, stopwords language, spaCy model, and the number of most common words to return. It processes the documents by tokenizing, lemmatizing, and removing stopwords, providing you with well-preprocessed documents and a list of the most common words within them.\n",
    "\n",
    "**When is it useful to use it?**\n",
    "\n",
    "This function is highly useful when you need to preprocess a collection of text documents for natural language processing tasks. It offers flexibility by allowing you to configure the spaCy pipeline according to your specific requirements. Additionally, it provides insights into the most common words in the preprocessed documents, which can be valuable for data validation or descriptive analysis. Let's explore a practical use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have a list of Spanish documents, and we want to preprocess them using the `spacy_pipeline` function with specific configuration options."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Tutorial: Topic Modelling\n",
    "**Introduction**\n",
    "\n",
    "In the age of social media, Twitter has become a fertile ground for data mining, sentiment analysis, and various other natural language processing (NLP) tasks. However, dealing with Spanish tweets adds another layer of complexity due to language-specific nuances, slang, abbreviations, and other colloquial expressions. 'TidyX' aims to streamline the preprocessing pipeline for Spanish tweets, making them ready for various NLP tasks such as text classification, topic modeling, sentiment analysis, and more. In this tutorial, we will focus on a classification task based on Topic Modelling, showing preprocessing, modeling and results with real data snippets.\n",
    "\n",
    "**Context**\n",
    "\n",
    "Using data provided by [Bar√≥metro de Xenofobia](https://barometrodexenofobia.org/), a world-class, renowned non-profit organization that quantifies the amount of hate speech against migrants on social media, we aim to classify the overall conversation related to migrants. This is a **common NLP task** that involves preprocessing poorly-written social media posts. Subsequently, these processed posts are fed into an unsupervised Topic Classification Model (LDA) to identify an optimal number of cluster topics. This helps reveal the main discussion points concerning Venezuelan migrants in Colombia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_terminal_size' from 'click.termui' (C:\\Users\\JOSE\\AppData\\Roaming\\Python\\Python39\\site-packages\\click\\termui.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ae0debcda90f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# PREPARATIONS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Environment set-up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtidyX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextPreprocessor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# Getting the data:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# In this tutorial, we use a sample dataset of tweets related to Venezuelan migrants in Colombia.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trabajo\\Paper_no_supervisado\\Tidytweets\\tidyX\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtidyX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextPreprocessor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextPreprocessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Trabajo\\Paper_no_supervisado\\Tidytweets\\tidyX\\TextPreprocessor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpanish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexplain\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\spacy\\cli\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwasabi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetup_cli\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapply\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0massemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0massemble_cli\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\spacy\\cli\\_util.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtyper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNoSuchOption\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msplit_arg_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\typer-0.3.2-py3.9.egg\\typer\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mecho_via_pager\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mecho_via_pager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0medit\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0medit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_terminal_size\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mget_terminal_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgetchar\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgetchar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlaunch\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlaunch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_terminal_size' from 'click.termui' (C:\\Users\\JOSE\\AppData\\Roaming\\Python\\Python39\\site-packages\\click\\termui.py)"
     ]
    }
   ],
   "source": [
    "# PREPARATIONS\n",
    "# Environment set-up\n",
    "import tidyX.TextPreprocessor as tt\n",
    "# Getting the data:\n",
    "# In this tutorial, we use a sample dataset of tweets related to Venezuelan migrants in Colombia.\n",
    "# The dataset is available in the data folder of the repository.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
