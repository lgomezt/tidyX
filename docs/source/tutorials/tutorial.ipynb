{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tidyX examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tidyX==1.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tidyX\n",
      "Version: 1.6.7\n",
      "Summary: Python package to clean raw tweets for ML applications\n",
      "Home-page: \n",
      "Author: Lucas GÃ³mez TobÃ³n, Jose Fernando Barrera\n",
      "Author-email: lucasgomeztobon@gmail.com, jf.barrera10@uniandes.edu.co\n",
      "License: MIT\n",
      "Location: c:\\users\\lucas\\anaconda3\\envs\\bx\\lib\\site-packages\n",
      "Requires: emoji, nltk, numpy, pandas, regex, spacy, thefuzz, Unidecode\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tidyX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidyX import TextPreprocessor as tp\n",
    "from tidyX import TextNormalization as tn\n",
    "from tidyX import TextVisualizer as tv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatizing Texts Efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stemmer()` and `lemmatizer()` functions each accept a single token as input. Thus, if we aim to normalize an entire text or a corpus, we would need to iterate over each token in the string using these functions. This approach might be inefficient, especially if the input contains repeated words.\n",
    "\n",
    "This tutorial demonstrates how to utilize the `unnest_tokens()` function to apply normalization functions just once for every unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @emilsen_manozca Â¿Me regala una moneda pa u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @CriptoNoticias Banco venezolano activa ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Capturado venezolano que asesinÃ³ a comerciante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @PersoneriaVpar @PersoneriaVpar acompaÃ±a al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bueno ya sacaron la carta de \"amenaza de atent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet\n",
       "0  RT @emilsen_manozca Â¿Me regala una moneda pa u...\n",
       "1  RT @CriptoNoticias Banco venezolano activa ser...\n",
       "2  Capturado venezolano que asesinÃ³ a comerciante...\n",
       "3  RT @PersoneriaVpar @PersoneriaVpar acompaÃ±a al...\n",
       "4  Bueno ya sacaron la carta de \"amenaza de atent..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, load a dataframe containing 1000 tweets from Colombia discussing Venezuela.\n",
    "tweets = tp.load_data(file = \"spanish\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @emilsen_manozca Â¿Me regala una moneda pa u...</td>\n",
       "      <td>regala moneda pa cafe venezolano no tuitero ah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @CriptoNoticias Banco venezolano activa ser...</td>\n",
       "      <td>banco venezolano activa servicio usuarios crip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Capturado venezolano que asesinÃ³ a comerciante...</td>\n",
       "      <td>capturado venezolano asesino comerciante merca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @PersoneriaVpar @PersoneriaVpar acompaÃ±a al...</td>\n",
       "      <td>acompa grupo especial migratorio cesar reunion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bueno ya sacaron la carta de \"amenaza de atent...</td>\n",
       "      <td>bueno sacaron carta amenaza atentado president...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  \\\n",
       "0  RT @emilsen_manozca Â¿Me regala una moneda pa u...   \n",
       "1  RT @CriptoNoticias Banco venezolano activa ser...   \n",
       "2  Capturado venezolano que asesinÃ³ a comerciante...   \n",
       "3  RT @PersoneriaVpar @PersoneriaVpar acompaÃ±a al...   \n",
       "4  Bueno ya sacaron la carta de \"amenaza de atent...   \n",
       "\n",
       "                                               clean  \n",
       "0  regala moneda pa cafe venezolano no tuitero ah...  \n",
       "1  banco venezolano activa servicio usuarios crip...  \n",
       "2  capturado venezolano asesino comerciante merca...  \n",
       "3  acompa grupo especial migratorio cesar reunion...  \n",
       "4  bueno sacaron carta amenaza atentado president...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Firstly we would clean the text easily using our preprocess function\n",
    "tweets['clean'] = tweets['Tweet'].apply(lambda x: tp.preprocess(x, \n",
    "                                                                delete_emojis = False, \n",
    "                                                                remove_stopwords = True, \n",
    "                                                                language_stopwords = \"spanish\"))\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will utilize the `unnest_token()` function to divide each tweet into multiple rows, assigning one token to each row. This structure allows us to aggregate identical terms, thereby creating an auxiliary dataframe that acts as a dictionary for lemmas or stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abajo</td>\n",
       "      <td>352, 577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandonar</td>\n",
       "      <td>337, 509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abarrotarse</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abiertos</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878</th>\n",
       "      <td>ðŸ¤ª</td>\n",
       "      <td>519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>ðŸ¤¬</td>\n",
       "      <td>483, 520, 908, 908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>ðŸ¤¯</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>ðŸ¤·</td>\n",
       "      <td>482, 736, 841, 947, 947, 947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>ðŸ¥º</td>\n",
       "      <td>833, 851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5883 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            clean                            id\n",
       "0                                           246\n",
       "1           abajo                      352, 577\n",
       "2       abandonar                      337, 509\n",
       "3     abarrotarse                           993\n",
       "4        abiertos                            72\n",
       "...           ...                           ...\n",
       "5878            ðŸ¤ª                           519\n",
       "5879            ðŸ¤¬            483, 520, 908, 908\n",
       "5880            ðŸ¤¯                           615\n",
       "5881            ðŸ¤·  482, 736, 841, 947, 947, 947\n",
       "5882            ðŸ¥º                      833, 851\n",
       "\n",
       "[5883 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_normalization = tp.unnest_tokens(df = tweets.copy(), input_column = \"clean\", id_col = None, unique = True)\n",
    "dictionary_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `id` column represents the indices of the tweets that contain each token from the `clean` column. Now we can proceed using the `stemmer()` and `lemmatizer()` functions to create new columns of `dictionary_normalization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply spanish_lemmatizer function to lemmatize the token\n",
    "dictionary_normalization[\"stemm\"] = dictionary_normalization[\"clean\"].apply(lambda x: tn.stemmer(token = x, language = \"spanish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to download the corresponding SpaCy model for lemmatization. For Spanish lemmatization, we suggest the `es_core_news_sm` model:\n",
    "\n",
    "```bash\n",
    "!python -m spacy download es_core_news_sm   \n",
    "```\n",
    "\n",
    "For English lemmatization, we suggest the `en_core_web_sm` model:\n",
    "\n",
    "```bash\n",
    "!python -m spacy download en_core_web_sm \n",
    "```\n",
    "\n",
    "To see a full list of available models for different languages, visit [Spacy's documentation](https://spacy.io/models/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "      <th>id</th>\n",
       "      <th>stemm</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>246</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abajo</td>\n",
       "      <td>352, 577</td>\n",
       "      <td>abaj</td>\n",
       "      <td>abajo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandonar</td>\n",
       "      <td>337, 509</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abarrotarse</td>\n",
       "      <td>993</td>\n",
       "      <td>abarrot</td>\n",
       "      <td>abarrotar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abiertos</td>\n",
       "      <td>72</td>\n",
       "      <td>abiert</td>\n",
       "      <td>abierto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878</th>\n",
       "      <td>ðŸ¤ª</td>\n",
       "      <td>519</td>\n",
       "      <td>ðŸ¤ª</td>\n",
       "      <td>ðŸ¤ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>ðŸ¤¬</td>\n",
       "      <td>483, 520, 908, 908</td>\n",
       "      <td>ðŸ¤¬</td>\n",
       "      <td>ðŸ¤¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>ðŸ¤¯</td>\n",
       "      <td>615</td>\n",
       "      <td>ðŸ¤¯</td>\n",
       "      <td>ðŸ¤¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>ðŸ¤·</td>\n",
       "      <td>482, 736, 841, 947, 947, 947</td>\n",
       "      <td>ðŸ¤·</td>\n",
       "      <td>ðŸ¤·</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>ðŸ¥º</td>\n",
       "      <td>833, 851</td>\n",
       "      <td>ðŸ¥º</td>\n",
       "      <td>ðŸ¥º</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5883 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            clean                            id    stemm      lemma\n",
       "0                                           246                    \n",
       "1           abajo                      352, 577     abaj      abajo\n",
       "2       abandonar                      337, 509  abandon  abandonar\n",
       "3     abarrotarse                           993  abarrot  abarrotar\n",
       "4        abiertos                            72   abiert    abierto\n",
       "...           ...                           ...      ...        ...\n",
       "5878            ðŸ¤ª                           519        ðŸ¤ª          ðŸ¤ª\n",
       "5879            ðŸ¤¬            483, 520, 908, 908        ðŸ¤¬          ðŸ¤¬\n",
       "5880            ðŸ¤¯                           615        ðŸ¤¯          ðŸ¤¯\n",
       "5881            ðŸ¤·  482, 736, 841, 947, 947, 947        ðŸ¤·          ðŸ¤·\n",
       "5882            ðŸ¥º                      833, 851        ðŸ¥º          ðŸ¥º\n",
       "\n",
       "[5883 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load model\n",
    "model_es = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Apply lemmatizer function to lemmatize the token\n",
    "dictionary_normalization[\"lemma\"] = dictionary_normalization[\"clean\"].apply(lambda x: tn.lemmatizer(token = x, model = model_es))\n",
    "\n",
    "# Lemmatizing could produce stopwords, therefore we applied remove_words function\n",
    "dictionary_normalization[\"lemma\"] = dictionary_normalization[\"lemma\"].apply(lambda x: tp.remove_words(x, remove_stopwords = True, language = \"spanish\"))\n",
    "\n",
    "dictionary_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To rebuild our original tweets we will use again `unnest_tokens` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>clean</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @emilsen_manozca Â¿Me regala una moneda pa u...</td>\n",
       "      <td>regala</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @emilsen_manozca Â¿Me regala una moneda pa u...</td>\n",
       "      <td>moneda</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @emilsen_manozca Â¿Me regala una moneda pa u...</td>\n",
       "      <td>pa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @emilsen_manozca Â¿Me regala una moneda pa u...</td>\n",
       "      <td>cafe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @emilsen_manozca Â¿Me regala una moneda pa u...</td>\n",
       "      <td>venezolano</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>RT infopresidencia: \"Sin lugar a dudas hay uno...</td>\n",
       "      <td>recibido</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>RT infopresidencia: \"Sin lugar a dudas hay uno...</td>\n",
       "      <td>cerca</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>RT infopresidencia: \"Sin lugar a dudas hay uno...</td>\n",
       "      <td>venezolanos</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>RT infopresidencia: \"Sin lugar a dudas hay uno...</td>\n",
       "      <td>presidente</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>RT infopresidencia: \"Sin lugar a dudas hay uno...</td>\n",
       "      <td>i</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13557 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweet        clean   id\n",
       "0    RT @emilsen_manozca Â¿Me regala una moneda pa u...       regala    0\n",
       "0    RT @emilsen_manozca Â¿Me regala una moneda pa u...       moneda    0\n",
       "0    RT @emilsen_manozca Â¿Me regala una moneda pa u...           pa    0\n",
       "0    RT @emilsen_manozca Â¿Me regala una moneda pa u...         cafe    0\n",
       "0    RT @emilsen_manozca Â¿Me regala una moneda pa u...   venezolano    0\n",
       "..                                                 ...          ...  ...\n",
       "999  RT infopresidencia: \"Sin lugar a dudas hay uno...     recibido  999\n",
       "999  RT infopresidencia: \"Sin lugar a dudas hay uno...        cerca  999\n",
       "999  RT infopresidencia: \"Sin lugar a dudas hay uno...  venezolanos  999\n",
       "999  RT infopresidencia: \"Sin lugar a dudas hay uno...   presidente  999\n",
       "999  RT infopresidencia: \"Sin lugar a dudas hay uno...            i  999\n",
       "\n",
       "[13557 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_long = tp.unnest_tokens(df = tweets.copy(), input_column = \"clean\", id_col = None, unique = False)\n",
    "tweets_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_x</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>lemma</th>\n",
       "      <th>stemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @emilsen_manozca Â¿Me regala una moneda pa u...</td>\n",
       "      <td>regalar moneda pa cafar venezolano  tuitero ah...</td>\n",
       "      <td>regal moned pa caf venezolan no tuiter ah ðŸ˜‚ ðŸ‘‹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @CriptoNoticias Banco venezolano activa ser...</td>\n",
       "      <td>banco venezolano activo servicio usuario cript...</td>\n",
       "      <td>banc venezolan activ servici usuari criptomoned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Capturado venezolano que asesinÃ³ a comerciante...</td>\n",
       "      <td>capturado venezolano asesino comerciante merca...</td>\n",
       "      <td>captur venezolan asesin comerci merc public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @PersoneriaVpar @PersoneriaVpar acompaÃ±a al...</td>\n",
       "      <td>acompa grupo especial migratorio cesar reunion...</td>\n",
       "      <td>acomp grup especial migratori ces reunion real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Bueno ya sacaron la carta de \"amenaza de atent...</td>\n",
       "      <td>bueno sacar cartar amenazar atentado president...</td>\n",
       "      <td>buen sac cart amenaz atent president duqu func...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_x                                              Tweet  \\\n",
       "0     0  RT @emilsen_manozca Â¿Me regala una moneda pa u...   \n",
       "1     1  RT @CriptoNoticias Banco venezolano activa ser...   \n",
       "2     2  Capturado venezolano que asesinÃ³ a comerciante...   \n",
       "3     3  RT @PersoneriaVpar @PersoneriaVpar acompaÃ±a al...   \n",
       "4     4  Bueno ya sacaron la carta de \"amenaza de atent...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  regalar moneda pa cafar venezolano  tuitero ah...   \n",
       "1  banco venezolano activo servicio usuario cript...   \n",
       "2  capturado venezolano asesino comerciante merca...   \n",
       "3  acompa grupo especial migratorio cesar reunion...   \n",
       "4  bueno sacar cartar amenazar atentado president...   \n",
       "\n",
       "                                               stemm  \n",
       "0      regal moned pa caf venezolan no tuiter ah ðŸ˜‚ ðŸ‘‹  \n",
       "1    banc venezolan activ servici usuari criptomoned  \n",
       "2        captur venezolan asesin comerci merc public  \n",
       "3  acomp grup especial migratori ces reunion real...  \n",
       "4  buen sac cart amenaz atent president duqu func...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_normalized = tweets_long \\\n",
    "    .merge(dictionary_normalization, how = \"left\", on = \"clean\") \\\n",
    "        .groupby([\"id_x\", \"Tweet\"])[[\"lemma\", \"stemm\"]] \\\n",
    "            .agg(lambda x: \" \".join(x)) \\\n",
    "                .reset_index()\n",
    "tweets_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Example 1\n",
      "Original tweet: RT @emilsen_manozca Â¿Me regala una moneda pa un cafÃ©? -Â¿Eres venezolano? Noo! Tuitero. -Ahhh ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ‘‹\n",
      "Lemmatized tweet: regalar moneda pa cafar venezolano  tuitero ah ðŸ˜‚ ðŸ‘‹\n",
      "Stemmed tweet: regal moned pa caf venezolan no tuiter ah ðŸ˜‚ ðŸ‘‹\n",
      "--------------------------------------------------\n",
      "Example 2\n",
      "Original tweet: RT @CriptoNoticias Banco venezolano activa servicio para usuarios de criptomonedas #ServiciosFinancieros https://t.co/1r2rZIUdlo\n",
      "Lemmatized tweet: banco venezolano activo servicio usuario criptomoneda\n",
      "Stemmed tweet: banc venezolan activ servici usuari criptomoned\n",
      "--------------------------------------------------\n",
      "Example 3\n",
      "Original tweet: Capturado venezolano que asesinÃ³ a comerciante del Mercado PÃºblico https://t.co/XrmWKVYMR8 https://t.co/CfMLaB25jI\n",
      "Lemmatized tweet: capturado venezolano asesino comerciante mercado publico\n",
      "Stemmed tweet: captur venezolan asesin comerci merc public\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"-\"*50)\n",
    "    print(\"Example\", i + 1)\n",
    "    print(\"Original tweet:\", tweets_normalized.loc[i, \"Tweet\"])\n",
    "    print(\"Lemmatized tweet:\", tweets_normalized.loc[i, \"lemma\"])\n",
    "    print(\"Stemmed tweet:\", tweets_normalized.loc[i, \"stemm\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    " \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_excel(r\"../../../data/Tweets sobre venezuela.xlsx\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all documents into a single string\n",
    "text = \" \".join(doc for doc in tweets['Snippet'])\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(background_color = \"white\", width = 800, height = 400).generate(text)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"WordCloud before tidyX\")\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean'] = tweets['Snippet'].apply(lambda x: tp.preprocess(x, delete_emojis = False, extract = False,\n",
    "                                                                  remove_stopwords = True))\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = tp.unnest_tokens(df = tweets.copy(), input_column = \"clean\", id_col = None, unique = True)\n",
    "token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy's model\n",
    "model = spacy.load('es_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply spanish_lemmatizer function to lemmatize the token\n",
    "token_df[\"lemma\"] = token_df[\"clean\"].apply(lambda x: tn.lemmatizer(token = x, model = model))\n",
    "token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df[\"lemma\"] = token_df[\"lemma\"].apply(lambda x: tp.remove_words(x, remove_stopwords = True))\n",
    "token_df = token_df[[\"clean\", \"lemma\"]]\n",
    "token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_long = tp.unnest_tokens(df = tweets.copy(), input_column = \"clean\", id_col = None, unique = False)\n",
    "tweets_long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean2 = tweets_long.merge(token_df, how = \"left\", on = \"clean\").groupby([\"Snippet\", \"id\"])[\"lemma\"].agg(lambda x: \" \".join(x)).reset_index()\n",
    "tweets_clean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean2['lemma'] = tweets_clean2['lemma'].apply(lambda x: tp.remove_extra_spaces(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all documents into a single string\n",
    "text = \" \".join(doc for doc in tweets_clean2['lemma'])\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(background_color = \"white\", width = 800, height = 400).generate(text)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"WordCloud after tidyX\")\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis(\"off\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
